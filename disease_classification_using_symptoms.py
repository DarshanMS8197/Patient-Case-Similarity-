# -*- coding: utf-8 -*-
"""disease_classification_using_symptoms.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1y5OetRthtWOST2QimsybFiZleqIkddM5
"""

# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)

# Input data files are available in the read-only "../input/" directory
# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory

import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using "Save & Run All"
# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session

"""# DISEASE CLASSIFICATION USING SYMPTOMS

![](https://c8.alamy.com/comp/R6NPE0/disease-word-cloud-design-R6NPE0.jpg)

# Importing Libraries

In this section, necessary libraries are imported to perform various tasks such as data manipulation, text preprocessing, machine learning, and visualization.
"""

# Importing libraries

import pandas as pd
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score, classification_report
import matplotlib.pyplot as plt
from wordcloud import WordCloud
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize

# Download nltk resources

nltk.download('punkt')
nltk.download('stopwords')

"""# Loading the Dataset

"""

# Load the dataset

data = pd.read_csv('Symptom2Disease.csv')

"""##### About the Dataset

The dataset consists of 1200 datapoints and has two columns: "label" and "text".

label : contains the disease labels
text : contains the natural language symptom descriptions.

The dataset comprises 24 different diseases, and each disease has 50 symptom descriptions, resulting in a total of 1200 datapoints.

The following 24 diseases have been covered in the dataset:

Psoriasis, Varicose Veins, Typhoid, Chicken pox, Impetigo, Dengue, Fungal infection, Common Cold, Pneumonia, Dimorphic Hemorrhoids, Arthritis, Acne, Bronchial Asthma, Hypertension, Migraine, Cervical spondylosis, Jaundice, Malaria, Urinary tract infection, Allergy, Gastroesophageal reflux disease, Drug reaction, Peptic ulcer disease, Diabetes

# Understanding the data
"""

# Displaying the dataset

data

data.drop(columns=["Unnamed: 0"], inplace=True)

data

# Concise summary of DataFrame

data.info()

# Check for null values

data.isnull().sum()

# Display column names

data.columns

data.value_counts()

# Extracting 'label' and 'text' columns from the 'data' DataFrame

labels = data['label']  # Contains the labels or categories associated with the text data
symptoms = data['text']  # Contains the textual data (e.g., symptoms, sentences) for analysis

"""# Text Preprocessing

Symptoms text is preprocessed, including tokenization, converting text to lowercase, and removing stopwords and non-alphabetic characters.
"""

# Text Preprocessing

stop_words = set(stopwords.words('english'))

# Text Preprocessing Function

def preprocess_text(text):
    # Tokenization
    words = word_tokenize(text.lower())
    # Removing stopwords and non-alphabetic characters
    words = [word for word in words if word.isalpha() and word not in stop_words]
    return ' '.join(words)

nltk.download('punkt_tab')

# Apply preprocessing to symptoms

preprocessed_symptoms = symptoms.apply(preprocess_text)

"""# Some Visualizations about the Data

## Word Cloud for symptoms:
"""

# Generate Word Cloud for symptoms
all_text = ' '.join(preprocessed_symptoms)
wordcloud = WordCloud(width=800, height=400, background_color='white').generate(all_text)


# Plotting Word Cloud
plt.figure(figsize=(10, 5))
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis('off')
plt.show()

""" ## Bar Chart for Disease Distribution:"""

import matplotlib.pyplot as plt

# Count the occurrences of each disease
disease_counts = data['label'].value_counts()

# Plotting a bar chart
plt.figure(figsize=(12, 6))
plt.bar(disease_counts.index, disease_counts.values, color='skyblue')
plt.xlabel('Disease')
plt.ylabel('Count')
plt.title('Distribution of Diseases')
plt.xticks(rotation=90)
plt.show()

"""## Word Cloud for Common Symptoms:"""

from wordcloud import WordCloud

# Combine all symptoms into a single string
all_text = ' '.join(preprocessed_symptoms)

# Generate Word Cloud
wordcloud = WordCloud(width=800, height=400, background_color='white').generate(all_text)

# Plot Word Cloud
plt.figure(figsize=(10, 5))
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis('off')
plt.show()

"""# Feature Extraction using TF-IDF

TF-IDF (Term Frequency-Inverse Document Frequency) vectors are generated from the preprocessed symptoms. This step converts text data into numerical features suitable for machine learning models.
"""

# Feature Extraction using TF-IDF

tfidf_vectorizer = TfidfVectorizer(max_features=1500)  # You can adjust max_features based on your dataset size
tfidf_features = tfidf_vectorizer.fit_transform(preprocessed_symptoms).toarray()

"""# Splitting Data

The dataset is split into training and testing sets, with 80% of the data used for training the model and 20% for testing its performance.
"""

# Split data into training and testing sets

X_train, X_test, y_train, y_test = train_test_split(tfidf_features, labels, test_size=0.2, random_state=42)

"""# KNN Model Training

A K-Nearest Neighbors (KNN) classifier is trained using the TF-IDF features and corresponding disease labels from the training set.
"""

# KNN Model Training

knn_classifier = KNeighborsClassifier(n_neighbors=5)  # You can adjust the number of neighbors (k) based on your dataset
knn_classifier.fit(X_train, y_train)

"""# Making Predictions

The trained KNN classifier is used to predict disease labels for the test set symptoms.
"""

# Predictions

predictions = knn_classifier.predict(X_test)

"""# Model Evaluation

The accuracy of the model is calculated using the predicted labels and compared with the actual labels. Additionally, a classification report is generated to provide detailed evaluation metrics.
"""

# Model Evaluation

accuracy = accuracy_score(y_test, predictions)
print(f'Accuracy: {accuracy:.2f}')
print(classification_report(y_test, predictions))

"""# Confusion Matrix for Model Evaluation"""

from sklearn.metrics import confusion_matrix
import seaborn as sns

# Generate confusion matrix
conf_matrix = confusion_matrix(y_test, predictions)

# Plotting confusion matrix as a heatmap
plt.figure(figsize=(10, 8))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=labels.unique(), yticklabels=labels.unique())
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix')
plt.show()

"""# Example Usage"""

# Example Usage
symptom = "Yellowing of skin and eyes, fatigue"

# Preprocess the input symptom
preprocessed_symptom = preprocess_text(symptom)

# Transform the preprocessed symptom using the same vectorizer used during training
symptom_tfidf = tfidf_vectorizer.transform([preprocessed_symptom])

# Predict the disease
predicted_disease = knn_classifier.predict(symptom_tfidf)
print(f'Predicted Disease: {predicted_disease[0]}')

